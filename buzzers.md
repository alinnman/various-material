# Indonesian Buzzer Networks Target Flat Earth Groups: The Economics of Algorithmic Manipulation

Indonesian "buzzer" networks—paid social media manipulators controlling an estimated **2.5 million fake accounts**—have evolved from political mercenaries into sophisticated engagement farmers [influenceindustry](https://influenceindustry.org/en/explorer/case-studies/indonesia-political-influence-operations/) targeting English-language conspiracy theory communities, with flat Earth content emerging as an optimal target. [arXiv +2](https://arxiv.org/html/2505.10867v1) These operations generate **15-fold higher revenue** by targeting English-speaking audiences compared to domestic Indonesian content, [uScreen +2](https://www.uscreen.tv/blog/youtube-cpm/) while the peculiar 14-day oscillating pattern between pro and anti positions directly exploits Facebook's algorithm testing cycles and content freshness windows. This represents a convergence of political disinformation infrastructure, click farm economics, and platform algorithm vulnerabilities that educators and moderators can learn to detect and disrupt. [arXiv](https://arxiv.org/html/2505.10867v1)

## The buzzer phenomenon has industrialized into a billion-dollar manipulation economy

Indonesian buzzers began as commercial marketing tools in 2009 but exploded into political warfare by 2012, reaching peak sophistication during the contentious 2019 presidential election. [Center for Strategic and International Studies +5](https://www.csis.org/blogs/new-perspectives-asia/democracy-digital-age-how-buzzer-culture-stinging-indonesias-democracy) Today, at least **10,000 Indonesians work as buzzers**, according to 2024 estimates from investigative outlet The Ken, operating through a sophisticated four-tier hierarchy. [Sage Journals](https://journals.sagepub.com/doi/10.1177/19401612241297832) [The Ken](https://the-ken.com/story/how-social-media-buzzers-scripted-indonesias-presidential-elections-under-meta-tiktok-xs-watch/) At the bottom, individual buzzers manage 10-300 fake accounts each, using tools like TweetDeck for semi-automated posting. Content creators craft memes and narratives serving client interests. Coordinators recruit buzzers, distribute content, and determine timing to generate trending topics. At the top, high-profile influencers lend credibility to campaigns while receiving payments to support specific narratives. [Inside Indonesia +3](https://www.insideindonesia.org/editions/edition-146-oct-dec-2021/organisation-and-funding-of-social-media-propaganda)

The financial scale is staggering. Indonesian government spending on digital procurement spiked from less than $40,000 in 2016 to **$34.6 million in 2017**, with a total of **$7.8 billion allocated between 2014-2020** [southeastasiaglobe](https://southeastasiaglobe.com/indonesia-election-buzzers/) [Southeast Asia Globe](https://southeastasiaglobe.com/indonesia-election-buzzers/) according to Indonesian Corruption Watch. [Cogitatio Press +2](https://www.cogitatiopress.com/mediaandcommunication/article/download/4225/2330) Individual operators report monthly turnover of $15,000, with profit margins around 80% yielding **$144,000 in annual profit** for mid-sized click farm operations. [madeinchinajournal](https://madeinchinajournal.com/2019/01/12/illicit-economies-of-the-internet-click-farming-in-indonesia-and-beyond/) Basic buzzers earn $3.50-$7 per account managed, while macro-influencers command $20 million+ for supporting presidential candidates, sometimes rewarded with commissioner positions at state enterprises. [influenceindustry +2](https://influenceindustry.org/en/explorer/case-studies/indonesia-political-influence-operations/)

ABC News's January 2024 investigation, published ahead of Indonesia's February 14 presidential election, captured growing international alarm about the phenomenon. Associate Professor Ika Idris of Monash University Indonesia told ABC News that buzzers function as "amplifiers" who boost posts using coordinated hashtags or create original posts carrying identical messages. [Monash University](https://www.monash.edu/indonesia/news/indonesias-social-media-buzzers-cashing-in-from-pushing-2024-election-propaganda) [insideindonesia](https://www.insideindonesia.org/beating-the-buzzers) The article highlighted how deeply normalized the practice has become, with some buzzer agency owners framing their businesses as legitimate "entrepreneurial ventures and digital marketing agencies." [Center for Strategic and International Studies](https://www.csis.org/blogs/new-perspectives-asia/democracy-digital-age-how-buzzer-culture-stinging-indonesias-democracy) [csis](https://www.csis.org/blogs/new-perspectives-asia/democracy-digital-age-how-buzzer-culture-stinging-indonesias-democracy) This normalization has facilitated the industry's expansion beyond electoral politics into commercial advertising, government policy defense, and increasingly, conspiracy theory propagation.

## Political disinformation infrastructure pivoted to conspiracy content after 2020

The evolution from political to conspiracy targeting represents strategic business diversification rather than ideological commitment. Between 2017-2019, buzzers became firmly entrenched in Indonesian politics, heavily contributing to religious and political polarization. [Sage Journals](https://journals.sagepub.com/doi/10.1177/19401612241297832) [insideindonesia](https://www.insideindonesia.org/beating-the-buzzers) The 2017 Jakarta gubernatorial election saw coordinated disinformation campaigns featuring manipulated video clips targeting Christian-Chinese incumbent Basuki Tjahaja Purnama (Ahok), contributing to his defeat and imprisonment on blasphemy charges. [Center for Strategic and International Studies +2](https://www.csis.org/blogs/new-perspectives-asia/democracy-digital-age-how-buzzer-culture-stinging-indonesias-democracy) By 2019, all major political candidates employed buzzer teams operating through WhatsApp coordination groups. [Cogitatio Press +2](https://www.cogitatiopress.com/mediaandcommunication/article/download/4225/2330)

After 2020, buzzer operations expanded dramatically into new domains. Government ministries hired buzzers to promote COVID-19 "New Normal" policies, defend the controversial Omnibus Law on Job Creation, and counter student protests by labeling demonstrators "destructive anarchists." [southeastasiaglobe +5](https://southeastasiaglobe.com/indonesia-election-buzzers/) Commercial clients openly advertise buzzer services on Instagram and e-commerce platforms like Shopee, selling followers, likes, and engagement packages. [Center for Strategic and International Studies +3](https://www.csis.org/blogs/new-perspectives-asia/democracy-digital-age-how-buzzer-culture-stinging-indonesias-democracy) This infrastructure—the same networks of fake accounts, automation tools, and coordination mechanisms built for political warfare—proved easily adaptable to conspiracy theory amplification.

Research by Kompas.id in 2023 found that buzzers position themselves as "information brokers" with exclusive "A1 information," and conspiracy theories are the **most widely circulated type of hoax** according to the Indonesian Anti-Defamation Society. In Indonesia's low-trust media environment where only 39% trust mainstream media according to Reuters 2021 data, buzzers fill an information vacuum. [Kompas +2](https://www.kompas.id/artikel/en-konspirasisme-buzzer-politik-dan-pemilu-2024) By 2025, research showed **37.4% of surveyed Indonesian investors had been exposed to economic conspiracy theories**, with 22.6% admitting these influenced investment decisions. [Frontiers](https://www.frontiersin.org/journals/human-dynamics/articles/10.3389/fhumd.2025.1617919/full) The same networks spreading false claims that COVID-19 was a Chinese lab experiment or that Indonesia's Corruption Eradication Commission was infiltrated by the Taliban now target flat Earth and other conspiracy communities. [insideindonesia](https://www.insideindonesia.org/editions/edition-146-oct-dec-2021/organisation-and-funding-of-social-media-propaganda)

## Flat Earth content offers the perfect combination of high engagement and low risk

Conspiracy theory content generates **2-11 times higher engagement** than factual news content according to multiple studies. [ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0022103122001408) A New York University Cybersecurity for Democracy study found conspiracy content receives nearly **11 times more predatory and deceptive ads** compared to mainstream content. [Cybersecurityfordemocracy](https://cybersecurityfordemocracy.org/understanding-monetization-youtube-conspiracy-theories) Research consistently shows conspiracy theories are "more viral, have longer lifespans and generate more user engagement" than debunking or scientific information. [PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC9483695/) People knowingly share conspiracy theories to advance social motives—receiving likes and comments—even when they know theories are false. [ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0022103122001408) [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3919364) The engagement value supersedes accuracy value in platform attention economies.

Flat Earth specifically offers unique advantages for buzzer operations. First, it attracts **passionate, engaged communities** with the vast majority of believers introduced through YouTube according to Texas Tech surveys. [ACM Digital Library](https://dl.acm.org/doi/fullHtml/10.1145/3485447.3512142) Second, it carries **minimal political risk**—unlike election interference or criticizing government policies, flat Earth content rarely triggers platform bans or law enforcement attention. Third, the community is **self-perpetuating**, with true believers creating free content that buzzers can amplify without significant production costs. Fourth, content naturally spreads across YouTube, Facebook, Twitter, Instagram, and TikTok, enabling cross-platform monetization. Fifth, despite periodic demonetization efforts, conspiracy content has "managed to stay on the platform and in the programme" according to YouTube Partner Program analyses. [PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC9483695/)

Platform algorithms actively favor this content. Social media algorithms "reward increased positive and negative engagement by directing traffic to posts and amplifying them" according to research on rage-baiting. [The Conversation +5](https://theconversation.com/how-engagement-makes-you-vulnerable-to-manipulation-and-misinformation-on-social-media-145375) YouTube's recommendation algorithm promotes conspiracy theories, making such content "popular amongst attention-seeking content creators." [PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC9483695/) Facebook's 2017 algorithm change to weight emoji reactions 5 times higher than likes created particular incentives—internal data showed posts triggering angry reactions were "disproportionately likely to include misinformation, toxicity and low-quality news." [Wikipedia +3](https://en.wikipedia.org/wiki/Rage-baiting) Each position a buzzer takes generates fresh controversy and algorithmic amplification.

## English-language targeting delivers fifteen times higher revenue than Indonesian content

The economic logic of targeting English-speaking audiences is overwhelming. YouTube CPM (cost per thousand impressions) rates for 2024-2025 show dramatic geographic disparities: United States audiences generate $6-40 CPM with an average of $15.34, United Kingdom $3-8 CPM, while Indonesian audiences generate less than $1-2 CPM and Pakistani audiences less than $1 CPM. [Is This Channel Monetized +4](https://isthischannelmonetized.com/data/youtube-cpm/) The same one million views generates **$8,430 in creator revenue** for US-targeted content compared to just $550-1,100 for Indonesian content—a **15-fold revenue difference**. [Shopify](https://www.shopify.com/blog/youtube-cpm) [Awisee](https://awisee.com/blog/youtube-cpm-rates/)

This economic reality shapes content strategy decisively. English content reaches 1.5 billion speakers globally versus 200 million Bahasa speakers. US, UK, and Australian advertisers pay premium rates, with finance, tech, and business content commanding $20-40 CPM. YouTube Premium revenue from Western subscribers generates $1-3 per 1,000 views. [Shopify](https://www.shopify.com/blog/youtube-cpm) Conspiracy channels are **twice as likely** to use offsite monetization including Patreon, GoFundMe, and PayPal donations, with **53% of demonetized channels** linking to third-party monetization platforms. [ACM Digital Library](https://dl.acm.org/doi/fullHtml/10.1145/3485447.3512142) [PubMed Central](https://pmc.ncbi.nlm.nih.gov/articles/PMC9483695/)

The Indonesian digital workforce possesses strong English skills despite only 15% of the general population speaking English. Young workers "learn English from YouTube videos and Discord servers" in self-taught models, and "Indonesian millennials prefer using English on social media" despite maintaining Indonesian identity. [SpringerOpen](https://telrp.springeropen.com/articles/10.1186/s41039-022-00198-8) This creates perfect conditions for labor arbitrage: Indonesian wage rates (Jakarta minimum wage around $320/month for factory work) combined with Western advertising rates. [influenceindustry](https://influenceindustry.org/en/explorer/case-studies/indonesia-political-influence-operations/) A successful operation running 10-50 YouTube channels simultaneously can generate **$1-5 million annually** according to economic modeling based on documented click farm revenues and YouTube monetization data. [madeinchinajournal](https://madeinchinajournal.com/2019/01/12/illicit-economies-of-the-internet-click-farming-in-indonesia-and-beyond/)

## The mysterious 14-day oscillation pattern exploits platform algorithm testing cycles

The pattern of accounts alternating between pro-flat-earth and anti-flat-earth content every 14 days initially appears bizarre, but it represents sophisticated algorithm gaming aligned with platform testing periods. Facebook and Instagram officially recommend running ads for **7-14 days minimum** before analyzing performance, according to platform documentation from SaveMyLeads and Social Insider. This timeframe "allows the Facebook algorithm to optimize your ads and gather enough data to provide meaningful insights." [Sprinklr](https://www.sprinklr.com/blog/social-media-algorithm/) [GRIN](https://grin.co/blog/always-be-testing-how-to-a-b-test-your-social-media-influencer-content/) A/B testing standards specify running tests "for a week or a full business cycle" minimum.

Critically, Facebook's algorithm tracks post age in distinct windows: **1-3 days old, 8-14 days old, and 14-21 days old**. Ranking signals include "how many times a user views posts that are either 1-3 days old, 8-14 days old, or 14-21 days old" according to Hootsuite Blog documentation of Facebook's algorithm. [Hootsuite Blog](https://blog.hootsuite.com/social-media-algorithm/) After 14 days, content becomes "stale" in algorithmic ranking. After 21 days, content rarely resurfaces organically. Switching positions every 14 days resets this engagement cycle, creating the appearance of fresh, evolving perspective rather than static propaganda while staying within optimal algorithmic visibility windows.

This oscillating strategy delivers multiple tactical advantages. First, it **captures both audiences**—pro-flat Earth believers one cycle, skeptics and debunkers the next—doubling potential reach. Second, each position switch brings new followers while previous followers engage to object, generating perpetual controversy. Third, it prevents algorithmic detection of coordinated inauthentic behavior by appearing organic rather than bot-like. Fourth, it enables **real-time A/B testing** of which messaging resonates better with audiences, optimizing future campaigns. Fifth, it builds perceived credibility by appearing "balanced" or "questioning" rather than ideological, making the account harder to dismiss as obvious propaganda.

Research on "rage farming" explains the underlying mechanism. Anger increases information-seeking behavior and drives web users to click political websites according to Journal of Politics research from 2012. [Wikipedia +2](https://en.wikipedia.org/wiki/Rage-baiting) Each position taken generates emotional responses from different audience segments. When an account switches from pro to anti flat Earth, previous supporters become critics, generating conflict that algorithms interpret as engagement signals worthy of amplification. Both sides sharing, quote-tweeting, and replying count identically as "engagement" in platform metrics, regardless of whether sentiment is positive or negative. [The Conversation](https://theconversation.com/how-engagement-makes-you-vulnerable-to-manipulation-and-misinformation-on-social-media-145375) [Substack](https://weaponizedspaces.substack.com/p/the-anatomy-of-a-viral-tweet-rage) The strategic value lies not in promoting specific beliefs but in **maximizing attention and engagement** through perpetual controversy generation.

## Both-sides investment hedges risks while maximizing market coverage

Buzzers invest in both sides of debates for sound business reasons. Research on platform economics shows "polarization and bias act as substitutes toward the platform's profit"—controversy generates sustained engagement that translates to revenue. [University of Kansas](https://news.ku.edu/news/article/profit-motivation-of-social-media-companies-may-compel-them-to-inject-bias-and-create-polarization-study-finds) [KU School of Business](https://business.ku.edu/news/article/profit-motivation-of-social-media-companies-may-compel-them-to-inject-bias-and-create-polarization-study-finds) By posting both pro and anti positions, operators achieve **maximum market coverage** while maintaining plausible deniability about their true positions or clients.

The hedge strategy provides risk mitigation. If one side loses political or social favor, the other compensates. Operators avoid being de-platformed for one-sided propaganda by maintaining an appearance of neutrality. Indonesian buzzers have demonstrated this flexibility historically—networks that worked for opposition Islamists during one campaign shifted to supporting President Jokowi's coalition when more profitable, with "yesterday's adversaries become today's allies and vice versa" according to academic documentation. [Sage Journals](https://journals.sagepub.com/doi/10.1177/19401612241297832) [influenceindustry](https://influenceindustry.org/en/explorer/case-studies/indonesia-political-influence-operations/)

Testing and optimization drive decisions. Both-sides posting enables identification of which narratives generate more engagement, which audience segments deliver highest CPM rates, and which content formats perform best—data that informs future campaigns. Client diversification becomes possible, with buzzer agencies simultaneously selling services to multiple political or ideological camps. The same operation can serve flat Earth believers seeking validation, debunkers seeking content to mock, and curious observers attracted by controversy, monetizing all three audiences simultaneously.

Documented cases show this pattern across Indonesian buzzer operations. The InsightID media firm network removed by Facebook in 2019 posted both pro and anti-independence content regarding West Papua, attempting to conceal identities while reaching 410,000 Facebook followers and spending $300,000 on ads. [The Hill](https://thehill.com/homenews/campaign/428091-facebook-takes-down-hundreds-of-accounts-linked-to-indonesian-fake-news/) [FB](https://about.fb.com/news/2019/10/removing-coordinated-inauthentic-behavior-in-uae-nigeria-indonesia-and-egypt/) The network wasn't advancing a position—it was farming engagement from all sides of a politically sensitive issue. Flat Earth content follows identical logic: both believers and debunkers drive views and engagement, and content creators profit regardless of position taken.

## Meta has removed thousands of Indonesian accounts but new networks continuously emerge

Facebook/Meta has conducted major enforcement actions against Indonesian coordinated inauthentic behavior since 2019, with the Saracen Group takedown representing the most significant documented case. In January 2019, Meta removed **207 Facebook Pages, 800 Facebook accounts, 546 Facebook Groups, and 208 Instagram accounts** connected to Saracen, reaching approximately 170,000 Facebook followers and 65,000+ Instagram followers. [The Daily Beast +8](https://www.thedailybeast.com/facebook-pulls-hundreds-of-indonesian-accounts-linked-to-fake-news-syndicate/) The Saracen Group gained infamy in 2017 when Indonesian police accused it of deliberately spreading hate speech and fake news for commercial purposes, with one member sentenced to 32 months in prison for "intentionally spreading information to incite hate." [The Daily Beast +5](https://www.thedailybeast.com/facebook-pulls-hundreds-of-indonesian-accounts-linked-to-fake-news-syndicate/)

Additional takedowns followed: 234 more Saracen-linked accounts in October 2019, [fb](https://about.fb.com/news/2019/01/taking-down-coordinated-inauthentic-behavior-in-indonesia/amp/) [FB](https://about.fb.com/news/2019/01/taking-down-coordinated-inauthentic-behavior-in-indonesia/) 145 accounts in a separate domestic network in April 2019 ahead of presidential elections, [Medium](https://medium.com/dfrlab/electionwatch-facebook-takes-down-network-supporting-indonesian-presidential-candidate-prabowo-18b8792e9529) 107 accounts focused on West Papua in December 2020, [fb +2](https://about.fb.com/news/2021/01/december-2020-coordinated-inauthentic-behavior-report/) and dramatically, **approximately 2,800 accounts, Groups, and Pages** in Q2 2022 for a novel "mass reporting" scheme. [FB +2](https://about.fb.com/news/2022/08/metas-adversarial-threat-report-q2-2022/) This last operation represented evolution in tactics—the network coordinated to weaponize Meta's reporting tools by falsely reporting people for violations including hate speech and impersonation, particularly targeting members of the Wahhabi Muslim community. [FB](https://about.fb.com/wp-content/uploads/2022/08/Quarterly-Adversarial-Threat-Report-Q2-2022.pdf) [FB](https://about.fb.com/news/2022/08/metas-adversarial-threat-report-q2-2022/) Accounts replaced letters with numbers when posting about targets to avoid detection and created fake accounts impersonating real people to report authentic users for impersonation. [FB](https://about.fb.com/wp-content/uploads/2022/08/Quarterly-Adversarial-Threat-Report-Q2-2022.pdf)

Meta's detection relies on behavioral analysis rather than content. Primary detection frameworks include temporal analysis examining timestamps to detect automation, network mapping identifying connections through group management and shared content strategies, and content similarity tracking identical posts across multiple accounts. Profile-level signals include fake profile pictures (GAN-generated faces, stock photos), simultaneous profile updates across accounts, and suspicious account age patterns. Network-level signals include centralized management where multiple pages manage the same groups, coordinated amplification of specific links, and posting synchronization within seconds across network nodes.

Despite these takedowns, Indonesian networks represent a persistent challenge. Meta noted "episodic spikes in false account creation from Indonesia, Nigeria, and Vietnam" in transparency reports. [Cloudfront](https://d18rn0p25nwr6d.cloudfront.net/CIK-0001326801/e574646c-c642-42d9-9229-3892b13aabfb.pdf) Indonesia has the 4th highest number of Facebook users globally with approximately 130 million users in 2022 (46.8% population penetration), creating scale challenges for detection. [DataReportal +2](https://datareportal.com/reports/digital-2022-indonesia) According to Meta's 2022 CIB enforcement recap, over 200 global networks have been disrupted since 2017 across 68 countries using 42 languages, though Indonesia is not listed among the top three prolific sources (Russia: 34, Iran: 29, Mexico: 13). [FB](https://about.fb.com/news/2022/12/metas-2022-coordinated-inauthentic-behavior-enforcements/amp/) The relatively lower documented removal rate may reflect either successful evasion or genuine lower volume compared to state-sponsored operations from autocracies.

## Detection requires recognizing behavioral patterns and coordinated timing

Educators, moderators, and individuals can identify buzzer accounts through systematic observation of red flags across three categories: profile indicators, behavioral patterns, and engagement anomalies. The "30-second profile check" method provides rapid initial assessment. Profile review checks whether the account is less than 6 months old, has an incomplete bio, uses a username with random numbers, displays a stock photo or avatar as profile picture, and lacks verification badges despite claiming public figure status. [Cyabra](https://cyabra.com/blog/how-to-spot-fake-social-media-profile/) Content scan examines whether the timeline shows only retweets and shares without original content, posts only memes or stock images, and lacks genuine personal narrative. [Better Business Bureau](https://www.bbb.org/all/spot-a-scam/how-to-spot-a-fake-social-media-account) [WHNT News 19](https://whnt.com/taking-action/bbb-consumer-alerts/how-to-spot-a-fake-social-media-account/) Engagement check notes whether comments are generic and bot-like, the account never responds authentically, and follower ratios show extreme imbalances. Three or more red flags indicate likely inauthenticity requiring deeper investigation. [Cyabra](https://cyabra.com/blog/how-to-spot-fake-social-media-profile/)

Behavioral indicators provide stronger evidence of coordination. Timing anomalies include accounts silent for months then suddenly posting multiple times per hour, or maintaining activity 24/7 without normal human breaks. [EU DisinfoLab](https://www.disinfo.eu/publications/cib-detection-tree1/) [Cyabra](https://cyabra.com/blog/how-to-spot-fake-social-media-profile/) The oscillating positions pattern—extremely positive or negative on topics with shifts to suit different campaigns—serves as a distinctive buzzer signature. Copy-paste content appears when multiple accounts share identical text simultaneously. Language inconsistencies mixing formal and informal Indonesian inappropriately or showing translation errors suggesting automated or foreign origin indicate manipulation.

Network-level investigation reveals coordination. Checking timestamps shows whether multiple accounts post identical content within seconds or minutes of each other. Tracking hashtag usage identifies whether the same accounts repeatedly push specific hashtags in coordinated campaigns. Examining who accounts follow and interact with exposes circular engagement patterns where accounts only engage with each other rather than organic users. Documenting shared group administration—where several fake profiles manage the same ten or more groups—provides strong evidence of centralized control, as Atlantic Council's Digital Forensic Research Lab identified in 2019 Saracen-linked networks. [medium](https://medium.com/dfrlab/electionwatch-facebook-takes-down-network-supporting-indonesian-presidential-candidate-prabowo-18b8792e9529) [Medium](https://medium.com/dfrlab/electionwatch-facebook-takes-down-network-supporting-indonesian-presidential-candidate-prabowo-18b8792e9529)

Free tools enable investigation without technical expertise. Google Reverse Image Search and TinEye verify profile pictures by identifying stock photos or images stolen from unrelated accounts. [West Virginia University](https://enews.wvu.edu/articles/2021/10/12/defend-your-data-use-these-tips-to-identify-fake-social-media-accounts) Twitter Advanced Search filters by date, location, and hashtags to track coordinated campaigns and find patterns in timing. FotoForensics detects image manipulation and extracts metadata showing when photos were taken and what device was used. Wayback Machine preserves historical versions of profiles to document account changes and find deleted content. Social Searcher monitors keywords in real-time across platforms. [PageFreezer](https://blog.pagefreezer.com/social-media-investigation-tools-for-socmint-investigations) These tools combined with systematic documentation through screenshots with timestamps provide evidence sufficient for platform reporting.

## Platform reporting mechanisms exist but require persistence and documentation

Meta's reporting process for Facebook and Instagram requires going to suspicious profiles or posts, clicking three dots in the top right corner, selecting "Report," and choosing appropriate categories including "Fake account" for individuals or "False information" for coordinated disinformation. [Norton](https://us.norton.com/blog/how-to/spot-fake-social-profile) Meta defines Coordinated Inauthentic Behavior as "coordinated efforts to manipulate public debate for a strategic goal where fake accounts are central to the operation." [Platformglossary +3](https://platformglossary.info/coordinated-inauthentic-behavior/) Reports should emphasize networks of fake accounts working together, accounts hiding real identities to mislead, coordinated posting of identical content, and mass harassment campaigns.

Effective reporting requires specific documentation. Screenshot accounts with timestamps before they're deleted or modified. Report specific posts and patterns, not just accounts, providing context on why content is misleading and how it coordinates with other accounts. Include evidence of coordinated behavior such as identical posting times, shared content, and network connections. Maintain records of your reports including submission dates and platform responses. Platforms typically review within 24-48 hours, with outcomes ranging from account removal for clear violations, reach restriction, monitoring for future violations, or no action if evidence is insufficient.

For Indonesian context, local resources supplement platform reporting. Mafindo (Indonesian Anti-Slander Society) accepts hoax reports at turnbackhoax.id and operates comprehensive fact-checking. [insideindonesia](https://www.insideindonesia.org/beating-the-buzzers) Cek Fakta provides multi-media fact-checking through Indonesian news organizations. [East Asia Forum +2](https://eastasiaforum.org/2019/07/05/indonesias-hoaxes-go-deeper-than-just-disinformation/) For serious cases involving illegal content, Indonesia's Ministry of Communication and Information handles reports, though critics note selective enforcement that sometimes targets government critics while ignoring pro-government cyber troops. [Cryptopolitan](https://www.cryptopolitan.com/indonesia-summons-tiktok-meta/) SAFENet (Southeast Asia Freedom of Expression Network) [insideindonesia](https://www.insideindonesia.org/beating-the-buzzers) and ICT Watch provide digital rights support and advocacy. [Inside Indonesia +2](https://www.insideindonesia.org/editions/edition-146-oct-dec-2021/beating-the-buzzers)

Persistence matters because platforms require multiple reports to act on sophisticated networks. Single reports rarely trigger removal of well-established accounts. Coordinated reporting from multiple legitimate users documenting the same network significantly increases enforcement likelihood. Researchers should share findings with organizations like the Atlantic Council's Digital Forensic Research Lab, Graphika, or academic researchers studying social media manipulation, who may have direct channels to platform trust and safety teams. The 2019 takedowns followed collaborative investigation involving DFRLab, Meta's internal teams, and open-source reporting from journalists. [Medium](https://medium.com/dfrlab/graphika-and-dfrlab-release-joint-report-operationffs-501d0cb31c5e)

## Educators can teach digital literacy through practical "spot the buzzer" exercises

Classroom integration of buzzer detection builds critical digital literacy skills. A 45-60 minute lesson plan begins with showing examples of buzzer accounts (with identifying information removed) and discussing implications for democracy and informed citizenship. The "spot the buzzer" game presents 3-5 example profiles for students to analyze, identifying red flags collaboratively. [insideindonesia](https://www.insideindonesia.org/beating-the-buzzers) Core skills training demonstrates reverse image search, teaches checking account age and activity patterns, and practices the 30-second profile check method.

The ESCAPE framework provides structured content evaluation: Evidence (do facts hold up?), Source (who made this, can I trust them?), Context (what's the big picture?), Audience (who is the target?), Purpose (why was this made?), and Execution (how is it presented?). [University of Oregon Libraries](https://researchguides.uoregon.edu/medialiteracy/evaluating-sources) Hands-on practice gives students real social media profiles to analyze using checklists identifying red flags, with findings shared in group discussion. Action steps cover reporting mechanisms and ethical considerations including the critical lesson that not all buzzers are malicious actors—some are legitimate influencers where transparency is the key issue.

Free educational resources support implementation. News Literacy Project's Checkology platform (newslit.org) provides interactive lessons on misinformation and fake accounts appropriate for grades 5 and up. Common Sense Education (commonsense.org/education) offers K-12 digital citizenship curriculum with lesson plans on identifying misinformation, filter bubbles, and echo chambers. Stanford History Education Group's Civic Online Reasoning curriculum (sheg.stanford.edu) teaches lateral reading—checking sources elsewhere before trusting content. [Democracy Toolkit](https://democracytoolkit.press/resources/teaching-verification/) MediaWise (mediawise.org) runs a Teen Fact-Checking Network blending online courses with workshops on real-time fact-checking techniques. [University of Oregon Libraries](https://researchguides.uoregon.edu/medialiteracy/further-readings)

Key teaching messages contextualize the phenomenon. Indonesian micro-buzzers earn Rp100,000-200,000 per post while macro-buzzers command Rp700,000-2,000,000, creating economic incentives. [Tempo](https://en.tempo.co/read/1280726/complex-vibrations-what-drives-political-buzzers) Indonesia's MUI (Ulama Council) declared negative buzzer activities "haram" (religiously forbidden), establishing moral stakes. [Inside Indonesia +2](https://www.insideindonesia.org/editions/edition-146-oct-dec-2021/beating-the-buzzers) Cyber troops degrade public discourse and threaten informed decision-making, making detection a civic responsibility. [PPIM UIN Jakarta +2](https://ppim.uinjkt.ac.id/2024/09/11/political-buzzer-networks-as-threat-to-indonesian-democracy/) During Indonesia's COVID-19 pandemic, demand for buzzer services increased 500%, demonstrating how crises create opportunities for manipulation that students must recognize and resist. [influenceindustry](https://influenceindustry.org/en/explorer/case-studies/indonesia-political-influence-operations/)

## The phenomenon reveals platform vulnerability to labor arbitrage and attention manipulation

Indonesian buzzer networks targeting flat Earth conspiracy groups represent more than isolated fraud—they expose fundamental vulnerabilities in platform attention economies and content moderation systems. [arXiv](https://arxiv.org/html/2505.10867v1) The convergence of Indonesian wage rates ($250-320/month minimum wage), Western advertising rates ($15+ CPM), algorithmic amplification of controversial content (2-11x engagement premium), and platform testing cycles (7-14 day optimization windows) creates economic incentives generating 400%+ returns on investment for sophisticated operators.

The global scale is substantial. Click farm and bot farm operations represent an estimated 10-20% of the **$41.4 billion in projected ad fraud losses for 2025**, suggesting $4-8 billion annually flows to these operations worldwide. [Spider AF](https://spideraf.com/articles/the-rise-of-click-farms-and-their-impact-on-digital-advertising-and-online-engagement) Indonesian operators capture significant market share through labor arbitrage and technical sophistication developed in political buzzing campaigns. The total addressable market continues growing as social media reaches 4.8 billion global users and advertising spend increases despite fraud concerns.

Platform responses have achieved partial success but face fundamental challenges. Meta's removal of over 200 coordinated inauthentic behavior networks globally since 2017 demonstrates commitment, [FB](https://about.fb.com/news/2022/12/metas-2022-coordinated-inauthentic-behavior-enforcements/amp/) but the "episodic spikes" in false account creation from Indonesia, Nigeria, and Vietnam show operators continuously adapt. [fb](https://about.fb.com/news/2021/01/december-2020-coordinated-inauthentic-behavior-report/) Detection systems increasingly use machine learning for pattern recognition, but sophisticated actors deliberately introduce irregularities mimicking organic behavior. The shift from fake accounts to compromised or authentic accounts used inauthentically (as seen in the 2022 mass reporting scheme) represents tactical evolution that platforms struggle to address without suppressing legitimate grassroots movements. [FB](https://about.fb.com/wp-content/uploads/2022/08/Quarterly-Adversarial-Threat-Report-Q2-2022.pdf) [FB](https://about.fb.com/news/2022/08/metas-adversarial-threat-report-q2-2022/)

The economic incentives remain powerful. Legitimate influencers can achieve 250-2,500% ROI by purchasing $50-200 worth of followers to attract $500-5,000 brand deals. Political campaigns spending $50,000-200,000 on buzzer operations can shift public opinion on policies or influence electoral outcomes, providing invaluable returns. Until platforms fundamentally restructure attention economies to reward accuracy over engagement, deprioritize rage and controversy in algorithmic ranking, and implement robust verification for monetization eligibility, the buzzer phenomenon will persist and evolve.

## Conclusion: Understanding industrial-scale manipulation strengthens educational responses

Indonesian buzzer networks targeting English-language flat Earth groups illuminate how political disinformation infrastructure adapts to exploit new opportunities in conspiracy communities, driven by platform economics that reward controversy regardless of veracity. The 14-day oscillation pattern represents sophisticated algorithm gaming rather than ideological commitment, exploiting Facebook's testing cycles while maximizing engagement from opposing audiences. The 15-fold revenue advantage of English content over Indonesian creates overwhelming incentives for labor arbitrage, as digitally skilled Indonesian workers target Western audiences with content designed to generate controversy-driven engagement.

For educators combating science misinformation, this research provides actionable intelligence. First, conspiracy theory communities face targeted manipulation by profit-motivated actors, not just sincere believers—skepticism about account authenticity is warranted. Second, the behavioral signatures of buzzer operations (oscillating positions, coordinated timing, copy-paste content, profile red flags) can be taught as practical detection skills. Third, the economic logic driving these operations helps students understand that "viral" content may reflect algorithmic manipulation rather than organic interest. Fourth, platform reporting mechanisms exist and become more effective when users document patterns systematically and report coordinated networks rather than individual posts.

The phenomenon ultimately demonstrates that science education must now encompass digital literacy and media manipulation awareness. [arXiv](https://arxiv.org/html/2505.10867v1) Students encountering flat Earth content online face not just misguided individuals but industrial-scale operations designed to generate engagement through controversy. Teaching "spot the buzzer" skills alongside scientific reasoning provides essential preparation for navigating information environments where attention economies reward manipulation. As long as platforms algorithmically amplify rage and controversy, and geographic wage disparities enable profitable labor arbitrage, educators must equip students to recognize when they're being manipulated—not just what to believe, but who is trying to shape their beliefs and why.
